[ft_instance_hyperparameter]
model_type=gpt
max_instance_num=4 ; Use for allocate the buffer
dynamic_batch_queue_wait_usecs = 10000
max_seq_len=128 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=3 ; k value for top k sampling
top_p=0.9 ; p value for top p sampling
temperature=1.0 ; Use for sampling
repetition_penalty=1.2 ; Use for sampling
presence_penalty=0.0  ; Only one of repetition_penalty and presence_penalty are allowed.
len_penalty=1.5
beam_search_diversity_rate=0.0
data_type=fp16
sparse=0
model_name=gpt2
int8_mode=1
tensor_para_size=1
pipeline_para_size=1
; model_name=megatron_345M
; model_name=megatron_6.7B
; model_name=gpt_175B
; model_name=self_defined
; model_dir=./models/megatron-models/c-model/6.7b/
; model_dir=models/openai-gpt-models/c-model/124m/1-gpu/
model_dir=/data/DeepDist/models/lora3.6B/lora/1-gpu
; model_dir=/data/DeepDist/models/gpt_cn/1-gpu
shared_contexts_ratio=1.0
enable_custom_all_reduce=0

[service]
tokenizer=bert
worker_instance_num=1

